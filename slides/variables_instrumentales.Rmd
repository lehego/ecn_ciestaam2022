---
title: "Variables instrumentales"
author: "Irvin Rojas"
header-includes:
  - \usepackage{tikz}
  - \usetikzlibrary{shapes, shadows,arrows}
  - \usepackage{amsmath} 
  - \usepackage[utf8]{inputenc}
output:
  xaringan::moon_reader:
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
        scroll: false
---

class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = F,
                      message = F, fig.path = "figures/")
library(tidyverse)
library(magick)
library(reticulate)
library(sandwich)
library(stargazer)
library(gtsummary)
library(foreign)
library(AER)
library(econocharts)
library(gmm)


xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))

knitr::opts_knit$set(root.dir = "C:/Users/rojas/Dropbox/presentations_sites/ecn_ciestaam2022")
```

.title[
# Variables instrumentales
]

.subtitle[
## Taller de Econometría CIESTAAM-UACh 2022
]

.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

---

# Agenda
  
1. Muchas de las aplicaciones empíricas en economía no cumplen con los supuestos de MCO sobre la exogeneidad de los regresores

1. Esto es de importancia cuando se buscan estimar relaciones causales, donde los parámetros estimados tengan una interpretación estructural

1. Crucialmente, hemos asumido que la exogeneidad en los regresores se cumple para utilizar LGN y TLC

1. Estudiaremos un método muy popular en econometría, usado ampliamente en la investigación aplicada para la identificación de parámetros en presencia de endogeneidad

---

class: inverse, middle, center

# Endogeneidad

---

# ¿Qué sucede si se violan los supuestos de MCO?

Supongamos que el el proceso de salarios **verdadero** está dado por

$$\ln(w_i)=\beta_0+\beta_1 educ_i+\beta_2 habilidad_i+e_i$$

Y asumamos que la habilidad es no observada y decidimos estimar

$$\ln(w_i)=\beta_0+\beta_1 educ_i+u_i$$

¿Dónde queda la habilidad?

El $\hat{\beta}_1$ estimado con esta regresión corta es inconsistente porque $u_i$ incluye la habilidad, que afecta tanto el desempeño en el mercado laboral como el desempeño en la escuela

---

# Instrumentos

Consideremos el siguiente modelo

$$y=\beta_0+\beta_1 x+u$$

donde $cov(x,u)\neq 0$

Suponga que existe una varible $z$ que cumple con:

  1. **Exogeneidad**: $z$ no está correlacionada con $u$ 
  $$cov(z,u)=0$$
  
  1. **Relevancia**: $z$ está correlacionado con $x$
  $$cov(z,x)\neq 0$$
  
Entonces $z$ es un **instrumento** de $x$


La exogeneidad implica que $z$ no debe estar correlacionado con factores omitidos (por ejemplo, la habilidad)

---

# Estimador de VI

Calculando la covarianza con $z$ de $y=\beta_0+\beta_1 x+u$ obtenemos:

$$cov(y,z)=\beta_1 cov(x,z)+cov(u,z)$$

Y, si $cov(u,z)$, resolviendo para $\hat{\beta}_1$

$$\hat{\beta}_1=\frac{cov(y,z)}{cov(x,z)}$$

siempre y cuando $cov(x,z)\neq 0$

Por una LGN se puede mostrar que $\hat{\beta}_1$ es consistente

Sin embargo, como profundizaremos más adelante, $\hat{\beta}_1$ siempre es sesgagado

El sesgo puede ser sustancial en muestras pequeñas, por lo que se recomienda tener precaución con el tamaño de la muestra

---

# Ejemplo: rendimientos a la educación

Card (1995) estudia el problema de retornos a la educación

Tenemos una muestra de 5,525 hombres de entre 14 y 24 años 

Nos interesa la relación entre educación e ingreso, pero sabemos que no observamos la habilidad

Sabemos que si estimamos $\ln(w_i)=\beta_0+\beta_1 educ_i+u_i$, el coeficiente $\beta_1$ estará sesgado

Card emplea como instrumento una variable $z_i$ que indica si en el municipio de la persona $i$ hay una universidad

La intución es que la presencia de la universidad baja el costo de ir a la universidad, pero esto no afecta directamente el ingreso

Encontrar un instrumento casi nunca es tarea sencilla: se trata de enteder cómo los mecanismos, las instituciones y los contextos


---

# Lenguaje de VI

De forma más general, partimos del la siguiente **ecuación estructural** para $y_i$:

$$y_i=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+u_i$$

donde $cov(x_{1i},u_i)\neq 0$

A $x_{1i}$ se le llama la variable **endógena**

Se incluye también una o más variables **exógenas** como $x_{2i}$ que no están correlacionadas con $u_i$

A una regresión de la variable de interés en función del instrumento y las variables exógenas se le conoce como **forma reducida**


$$y_i=\beta_0+\beta_1 z_{1i}+\beta_2 x_{2i}+u_i$$

---

# Primera etapa

La **primera etapa** especifica la relación entre la variable endógena y el instrumento:

$$x_{1i}=\pi_0+\pi_1 z_i+\pi_2x_{2i}+\nu_i$$

Donde se cumple que $cov(z_i,\nu_i=0)$ y $cov(x_{2i},\nu_i)=0$

Entonces, la condición de relevancia puede escribirse también como $\pi_1\neq 0$

Noten que la primera etapa también implica que, descontando el efecto de $z_i$, todavía $x_{1i}$ y $x_{2i}$ están correlacionadas

La primera etapa puede y **debe** probarse empíricamente

En cambio, no es posible probar la restricción de exclusión, que debe estar respaldada sobre todo por la teoría económica, el conocimiento de las instituciones, la exogeneidad de experimentos naturales, etc.
  
---

# Más de un instrumento

Es posible que haya $J$ variables $z_{ij}$ que puedan funcionar como instrumento

Se debe cumplir que $cov(u_i,z_{ij})=0$ y que cada una se correlacione con $x_{i1}$

Con dos instrumentos, podemos escribir la primera etapa como

$$x_{1i}=\pi_0+\pi_1 z_{1i}+ \pi_2 z_{2i} +\pi_3x_{2i}+\nu_i$$

Ahora, debe cumplirse que $cov(z_{1i},\nu_i)=cov(z_{2i},\nu_i)=cov(x_{2i},\nu_i)=0$

Para lograr identificación, se requiere que $\pi_1\neq 0$ o $\pi_2\neq 0$

Podemos usar una prueba $F$ para probar que $\pi_1=\pi_2=0$

---

# Mínimos cuadrados en dos etapas

El modelo presentado anteriormente sugiere que podemos estimar $\beta_1$ con un procedimiento de dos etapas

  1. Regresión de $x_{1i}$ sobre los instrumentos y las variables exógenas para obtener $\hat{x}_{1i}$
  
  1. Regresión de $y_i$ sobre las variables exógenas y $\hat{x}_{1i}$
  
Es como si *purgáramos* a $x_{1i}$ de su correlación con $u_i$

Nunca hacemos esto *a mano*

- Cuando tenemos tantos instrumentos como endógenas, usamos el **estimador de variables instrumentales**

- Cuando tenemos más instrumentos que endógenas, recurrimos al método generalizado de momentos

---

# Más sobre rendimientos a la educación

En el problema de Card (1995), la primera etapa es

$$esc_i=\pi_0+\pi_1 X_i+ \phi unicerca_i +\nu_i$$
donde $unicerca_i= \begin{cases} 1 \quad\text{había una universidad en el municipio} \\ 0 \quad\text{otro caso}\\ \end{cases}$

Y la forma reducida es


$$\ln(w_i)=\gamma_0+\gamma_1 X_i+  \delta unicerca_i+\varepsilon_i$$

Sabemos que el salario estará correlacionado con la presencia de la universidad, pero estas diferencias ocurren por la vía de la escolaridad


---

# Ejemplo: Card (1995)

Usamos los datos en *ingresos_iv.csv*, del estudio de Card que hemos mencionado como ejemplo

La librería *AER*, que ya hemos usado, tiene la función *ivreg*

También usaremos una nueva librería, *gmm*

Estimemos la relación entre el log del salario y la educación

```{r echo=T}
data.ingresos <- read_csv("data/ingresos_iv.csv",
                          locale = locale(encoding = "latin1"))
#MCO
mco <- lm(lwage ~ educ + exper + black + south + married + smsa,
          data = data.ingresos)

#Variables instrumentales (asume homocedasticidad)
vi <- ivreg(lwage ~  educ + exper + black + south + married + smsa |
               . - educ + nearc4, data = data.ingresos)

```
---

# Ejemplo: Card (1995)

Sabemos que $\beta_1$ estimado por MCO es inconsistente

```{r results='asis'}
stargazer(mco, vi,
          type="html",
          keep=c("educ"),
          keep.stat = "n")
```
---

# Ejemplo: Card (1995)

.pull-left[
La primera etapa de este ejercicio es

```{r echo=T}
# Primera etapa
pe_vi <- lm(educ ~  nearc4 + exper + black + south + married + smsa,
            data = data.ingresos)
```
]
.pull-right[
```{r echo=F, results='asis'}
stargazer(mco, vi, pe_vi,
          type="html",
          keep=c("educ", "nearc"),
          keep.stat = "n")
```
]

---

# Ejemplo: Card (1995)

Debemos verificar el estadístico $F$ de la primera etapa (para los instrumentos)

```{r echo=T}
#F de los instrumentos
linearHypothesis(pe_vi, c("nearc4=0"))
```

---

# Ejemplo con oferta y demanda

Supongamos que nos interesa estimar la elasticidad del consumo de mantequilla

$$\ln(q_{_i})=\alpha + \beta \ln(p_i) + \varepsilon_i$$
Supongamos que tenemos datos del precio y el consumo en una muestra grande de localidades $i$

Si estimamos la ecuación anterior por MCO, el coeficiente estimado $\hat{\beta}$ será inconsistente por un problema de simulateneidad

El precio y la cantidad se determinan en equilibrio por la interacción de la oferta y la demanda

Un choque a la oferta o la demanda afectará tanto la cantidad como el precio de equilibrio

---

# Ejemplo con oferta y demanda

.pull-left[

Podemos usar variables instrumentales para estimar la elasticidad de la demanda

Necesitamos una variable $z$ que afecte solo la oferta, pero que no afecte directamente la demanda

$z$ puede ser precipitación o temperatura

Al desplazar la oferta, manteniendo la demanda fija, se revela la forma de la curva de demanda

Podemos estimar la elasticidad de la demanda
]

.pull-right[
```{r echo=F, results=F, out.height=400}
# Add custom curves
demand1 <- data.frame(Hmisc::bezier(c(1, 3, 9),
                                    c(9, 3, 1))) 

supply1 <- data.frame(Hmisc::bezier(c(1, 8, 9),
                                    c(1, 5, 9))) 

supply2 <- data.frame(Hmisc::bezier(c(1, 8, 9),
                                    c(3, 8, 12))) 

supply3 <- data.frame(Hmisc::bezier(c(1, 8, 9),
                                    c(5, 10, 14))) 


# Supply and demand curves and arrows
sdcurve(supply1, demand1, supply2, demand1, supply3, demand1,
        names = c("S[1]", "D[1]","S[2]", "D[1]", "S[3]", "D[1]"),
        xmax = 15, ymax=10)
```
]

---

class: inverse, middle, center

# Método Generalizado de Momentos

---


# Método generalizado de momentos

El GMM generaliza una serie de estimadores comúnmente usados en econometría (incluyendo MCO, MV, VI, etc)

Asumimos que existen $r$ condiciones de momentos independientes para $q$ parámetros $$E(h(w_i,\theta_0))=0$$

donde $\theta$ es un vector de $q\times 1$, $h(\cdot)$ es una función vector de $r \times 1$ con $r\geq q$

$w_i$ son los datos observables, incluyendo las variables dependientes, los regresores exógenos, potenciales regresores endógenos, así como instrumentos


---

# Método generalizado de momentos

La forma de $h(\cdot)$ es equivalente a escoger el modelo


Por ejemplo:

| $h(\cdot)$ | Método de estimación |
|:---:|:---:|
| $x(y-x'\beta)$ | MCO |
| $\partial\mathcal{L}/\partial\theta$ | MV |
| $z(y-x'\beta)$ | VI |

---

# Método generalizado de momentos

Cuando $r=q$, tenemos un modelo **exactamente identificado**, es decir, tenemos tantos momentos como parámetros a estimar

Podemos obtener el **estimador de método de momentos** $\hat{\theta}_{MM}$ como la solución a

$$\frac{1}{N}h(w_i,\hat{\theta})=0$$

---

# Método generalizado de momentos

El caso que nos ocupa más en el contexto de MC2E es cuando $r>q$, es decir, un **modelo sobreidentificado**

En este caso, tenemos más ecuaciones que incógnitas en la condición de momentos

El **estimador de método generalizado de momentos** $\hat{\theta}_{GMM}$ se define como el vector de parámetros que minimiza la forma cuadrática

$$Q_N(\theta)=\left(\frac{1}{N}\sum_ih(w_i,\theta)\right)'W_N\left(\frac{1}{N}\sum_ih(w_i,\theta)\right)$$

donde $W_N$ es una matriz simétrica y positiva definida que no depende de $\theta$

Diferentes matrices $W_N$ dan origen a distintos estimadores

---

# Estimador de MGM

**Proposición 6.1 en CT**: bajo una serie de supuestos para poder establecer LGN y TLC, $\hat{\theta}_{GMM}$, definido como una raíz de las condiciones de primer orden $\partial Q_N(\theta) / \partial \theta=0$, es tal que:

$$\sqrt{N}\left(\hat{\theta}_{GMM}-\theta_0\right)\stackrel{a}{\sim}\mathcal{N}\left(0,(G_0'W_0G_0)^{-1}(G_0'W_0S_0W_0G_0)(G_0'W_0G_0)^{-1}\right)$$

donde $W_0$ es una matriz finita, simétrica y positiva definida, y

$$
\begin{aligned}
G_0&=p\lim\frac{1}{N}\sum_{i=1}^N \left(\frac{\partial h_i}{\partial\theta'}\Bigg|_{\theta_0}\right) \\
S_0&=p\lim \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N \left(h_i h_j \Bigg|_{\theta_0} \right)
\end{aligned}
$$
---

# Matriz de varianzas óptima

Para implementar MGM debemos especificar las condiciones de momentos y la matriz $W_N$

En el caso de modelos sobreidentificados y con $S_0$ conocida, el estimador de MGM más eficiente se obtiene al especificar $W_N=S_0^{-1}$

 Con esta elección, la expresión para la varianza de $\hat{\beta}_{MGM}$ se simplifica a
 
$$\sqrt{N}\left(\hat{\theta}_{GMM}-\theta_0\right)\stackrel{a}{\sim}\mathcal{N}\left(0,(G_0'S_0^{-1}G_0)^{-1}\right)$$

En la práctica, $S_0$ es desconocida, así que la sustituimos por un estimador consistente $\hat{S}$


---

# MGM óptimo

La matriz de varianzas se estima siguiendo un procedimiento de dos etapas

1. Obtener el estimador de MGM usando una matriz subóptima, generalmente $W_N=I_r$ y con estos coeficientes obtener un estimador para $S_0$: $$\hat{S}=\frac{1}{N}\sum_i h_i(\hat{\theta})h_j(\hat{\theta})'$$
  
1. Obtener un estimador de MGM óptimo o **estimador de MGM de dos etapas óptimo** $\hat{\theta}_{MGM,O}$ minimizando
  
$$Q_N(\theta)=\left(\frac{1}{N}\sum_ih(\theta)\right)'\hat{S}^{-1}\left(\frac{1}{N}\sum_ih(\theta)\right)$$

---

# MGM óptimo

Para estimar la varianza de $\hat{\theta}_{MGM,O}$ usamos

$$\hat{V}(\hat{\theta}_{MGM,O})=N^{-1}(\hat{G}\tilde{S}^{-1}\hat{G})^{-1}$$

donde $\hat{G}$ y $\tilde{S}$ se evalúan en $\hat{\theta}_{MGM,O}$


---

class: inverse, middle, center

# MGM para VI

---

# Estimador lineal de MGM

Cuando estamos en un problema de VI, la restricción de exclusión nos especifica una condición de momentos

$$E(z_i(y_i-x_i'\beta))=0$$

El estimador GMM minimiza la forma cuadrática siguiente

$$
\begin{aligned}
Q(\beta)&=\left(\frac{1}{N}\sum_i (y_i-x_i'\beta)z_i\right)'W_N\left(\frac{1}{N}\sum_i (y_i-x_i'\beta)z_i\right) \\
&=\left(\frac{1}{N}(y-X\beta)'Z\right)W_N\left(\frac{1}{N}Z'(y-X\beta)\right)
\end{aligned}
$$

Las condiciones de primer orden son

$$\frac{\partial Q_N(\beta)}{\partial\beta}=-2\left(\frac{1}{N}X'Z\right)W_N\Big(\frac{1}{N}Z'(y-X\beta)\Big)=0$$

---

# Estimador lineal de MGM

Resolviendo para $\beta$ obtenemos el **estimador lineal de VI de GMM**:

$$\hat{\beta}_{GMM}=(X'ZW_NZ'X)^{-1}X'ZW_NZ'y$$

Las propiedades asintóticas de este estimador se pueden obtener de manera similar a como se obtuvieron las del estimador de MCO o usando las propiedades más generales para problemas de MGM

---

# Estimador de la varianza de $\hat{\beta}_{MGM}$

El estimador $\hat{\beta}_{MGM}$ es asintóticamente normal, centrado en $\beta$ y con una varianza asintótica estimada dada por

$$\hat{V}(\hat{\beta}_{GMM})=N(X'ZW_NZ'X)^{-1}(X'ZW_N\hat{S}W_NZ'X)(X'ZW_NZ'X)^{-1}$$
donde $\hat{S}$ es un estimador consistente de

$$S=\lim \frac{1}{N}\sum_{i=1}^NE(u_i^2z_iz_i')$$

Dependiendo de si estamos en un modelo exactamente identificado o sobreidentificado y de cómo especificamos la matriz $W_N$, los resultados anteriores sobre $\hat{\beta}_{GMM}$ y $\hat{V}(\hat{\beta}_{GMM})$ se especializan

---

class: inverse, middle, center

# Caso sobre identificado

---

# Estimador óptimo de MGM

Para obtener el **estimador óptimo** escogemos una forma particular para la matriz de pesos

$$W=\hat{S}^{-1}$$

Y entonces el estimador de MGM se vuelve

$$\hat{\beta}_{GMM,O}=(X'Z\hat{S}^{-1}Z'X)^{-1}X'Z\hat{S}^{-1}Z'y$$

Y el estimador de varianza se simplifica a

$$\hat{V}(\hat{\beta}_{GMM,O})=N(X'Z\hat{S}^{-1}Z'X)^{-1}$$

Hasta aquí no asumimos nada sobre la forma de los errores

Lo único que nos permitió pasar de la forma general al estimador óptimo es la elección de $W$

Con esto obtenemos el estimador más eficiente

---

# Mínimos cuadrados en dos etapas

Si estamos dispuestos a asumir errores homocedásticos

$$\hat{S}^{-1}=\left(\frac{1}{N}s^2Z'Z\right)^{-1}$$

Y entonces hacemos

$$W=\left(\frac{1}{N}Z'Z\right)^{-1}$$

Con esta simplificación, el estimador de MGM es

$$
\begin{aligned}
\hat{\beta}_{MC2E}&=(X'Z(Z'Z)^{-1}Z'X)^{-1}X'ZZ(Z'Z)^{-1}Z'y \\
&=(X'P_ZX)^{-1}X'P_Zy
\end{aligned}
$$

Este es el **estimador de MC2E**, también llamado **estimador de variables instrumentales generalizado**

---

# Mínimos cuadrados en dos etapas

A $P_Z=Z(Z'Z)^{-1}Z'$ se le conoce como matriz de proyección

Y la matriz de varianzas se simplifica a

$$\hat{V}(\hat{\beta}_{MC2E})=s^2\left(X'P_z X\right)^{-1}$$

---

class: inverse, middle, center

# Caso exactamente identificado

---

# Estimador de variables instrumentales

En el caso cuando $r=q$, es decir, tantos instrumentos como variables endógenas, $X'Z$ es una matriz cuadrada que puede ser invertida, resultando que

$$(X'ZW_NZ'X)^{-1}(X'Z)^{-1}=(Z'X)^{-1}W_N(X'Z)^{-1}$$


Sustituyendo esto en la forma general del estimador de MGM obtenemos:

$$\hat{\beta}_{VI}=(Z'X)^{-1}Z'y$$

Este es el estimador de **variables instrumentales**

En otras palabras, el estimador de MGM es igual al de VI para cualquier matriz $W_N$

---

# Modelo exactamente identificado

Con posible heterocedasticidad, tenemos una matriz de varianzas de la forma

$$\begin{align}\hat{V}(\hat{\beta}_{VI})&=N(Z'X)^{-1}\hat{S}(X'Z)^{-1}\end{align}$$

con $\hat{S}=Z'DZ/N$, y donde $D=diag[\hat{u}_i^2]$
 

Y con homocedasticidad, la matriz de varianzas de nuevo es:

$$\hat{V}(\hat{\beta}_{VI})=s ^2\left(X'P_z X\right)^{-1}$$

---

# Recapitulando

Siguiendo las convenciones de Camerony Trivedi (2005)

El **estimador de MGM** es el estimador para el caso general de método de momentos, cuales quiera que sean las formas de los momentos especificados

El **estimador óptimo de MGM** ocurre cuando asumimos una forma particular para la matriz de pesos, $W=\hat{S}^{-1}$

El **estimador óptimo de MGM** se emplea en el caso más general de modelos de variables instrumentales sobreidentificados con heterocedasticidad

El **estimador de variables instrumentales generalizado** se obtiene cuando asumimos homocedasticidad en el modelo sobreidentificado y lleva el *apellido* generalizado porque es la generalización del estimador IV para el caso sobreidentificado

El **estimador de variables instrumentales** surge en el modelo exactamente identificado

---

# Ejemplo: Card (1995)

Usando *gmm* podemos verificar que IV es un caso especial de GMM

```{r echo=T}
gmm_iv <- gmm(lwage ~ educ + exper + black + south + married + smsa, 
                   ~ nearc4 + exper + black + south + married + smsa,
                   vcov = "iid",
                   wmatrix = "optimal", # es igual si usamos optimal
                   data = data.ingresos)

```
---

# Ejemplo: Card (1995)


```{r results='asis'}
stargazer(vi, gmm_iv,
          type="html",
          keep = c("educ"),
          digits = 4,
          keep.stat = "n")
```
---

# Ejemplo: Card (1995)

Con GMM podemos relajar fácilmente el supuesto de homocedasticidad

```{r echo=T}
gmm_iv_het <- gmm(lwage ~ educ + exper + black + south + married + smsa, 
                  ~ nearc4 + exper + black + south + married + smsa,
                  vcov = "HAC",
                  wmatrix = "optimal",
                  type = "twoStep",
                  data = data.ingresos)
``` 
---

# Ejemplo: Card (1995)

```{r results='asis'}
stargazer(vi, gmm_iv, gmm_iv_het,
          type="html",
          keep = c("educ"),
          digits = 4,
          keep.stat = "n")

``` 
---

# Ejemplo: Card (1995)

Podemos especificar un modelo sobreidentificado

```{r echo=T}
gmm_opt <- gmm(lwage ~ educ + exper + black + south + married + smsa, 
                  ~ nearc4 + nearc2 + exper + black + south + married + smsa,
                  vcov = "HAC",
                  wmatrix = "optimal",
                  type = "twoStep",
                  data = data.ingresos)
```

---

# Ejemplo: Card (1995)

```{r results='asis'}
stargazer(vi, gmm_iv, gmm_iv_het, gmm_opt,
          type="html",
          keep = c("educ"),
          digits = 4,
          keep.stat = "n")
```


---

class: inverse, middle, center

# Prueba de Hausman

---

# Prueba de Hausman

En general, las pruebas que comparan dos estimadores distintos se conocen como pruebas de Hausman, Wu-Hausman o Durbin-Wu-Hausman

Consideremos dos estimadores $\tilde{\theta}$ y $\hat{\theta}$ que tienen la misma probabilidad límite bajo la $H_0$ pero que difieren bajo la $H_a$

$$
\begin{aligned}
H_0:\quad\quad p\lim(\tilde{\theta}-\hat{\theta})=0 \\
H_a:\quad\quad p\lim(\tilde{\theta}-\hat{\theta})\neq 0 \\
\end{aligned}
$$

Construimos el estadístico de prueba $H$:

$$H=(\tilde{\theta}-\hat{\theta})'(\hat{V}(\tilde{\theta}-\hat{\theta}))^{-1}(\tilde{\theta}-\hat{\theta})\stackrel{a}{\sim}\chi^2(q)$$

Se rechaza la $H_0$ si $H>\chi^2_{\alpha}(q)$

La implementación es un poco complicada dado que

$$\hat{V}(\tilde{\theta}-\hat{\theta})=\hat{V}(\tilde{\theta})-\hat{V}(\hat{\theta})-2cov(\tilde{\theta},\hat{\theta})$$

---

# Prueba de Hausman

Con errores homocedásticos, el estimador de MCO es eficiente

En ese caso, se puede mostrar que

$$H_{h}=(\tilde{\theta}-\hat{\theta})'(\hat{V}(\tilde{\theta})-\hat{V}(\hat{\theta}))^{-1}(\tilde{\theta}-\hat{\theta})\stackrel{a}{\sim}\chi^2(q)$$
que es fácil de calcular en el software

Si no estamos dispuestos a asumir homocedasticidad, se requiere estimar $cov(\tilde{\theta},\hat{\theta})$, que se implementa en R y otros paquetes

La prueba de Hausman puede usarse para comparar dos estimadores, uno más eficiente que otro

La estimación de la prueba robusta puede complicarse en algunas aplicaciones, aunque como prueba de endogeneidad casi todo está disponible como funciones en R y otros paquetes

---

class: inverse, middle, center

# Prueba de sobreidentificación

---

# Prueba de sobreidentificación

También conocida como prueba de Hansen, quien propuso la forma general de la prueba, o prueba de Sargan, quien propuso la forma particular para el modelo lineal de VI

Es una prueba sobre qué tan cerca está de cumplirse la hipótesis nula de que $E(h(w,\theta_0))=0$

Hansen (1982) define el estadístico de prueba como

$$J=\left(\frac{1}{N}\sum_i \hat{h}_i\right)'\hat{S}^{-1}\left(\frac{1}{N}\sum_i \hat{h}_i\right)\stackrel{a}{\sim}\chi^2(r-q)$$

El estadístico $J$ es la función objetivo de MGM evaluada en $\hat{\theta}_{MGM}$

Si el estadístico es grande en magnitud, rechazamos la hipótesis de que las condiciones de momentos poblacionales se cumplen y se concluye que el estimador de MGM es inconsistente

---

# Prueba de sobreidentificación

En el caso de variables instrumentales, el estadístico tiene la forma específica:

$$J=\hat{u}'Z\hat{S}^{-1}Z'\hat{u}$$
donde $\hat{u}=y-X'\hat{\beta}_{MGM}$

Si se rechaza $H_0$, hay evidencia de que los instrumentos $z$ son endógenos (aunque también podría ser que haya una mala especificación del modelo)

Rechazar la $H0$ indica que debemos replantear el modelo, aunque no nos dice cómo

---

# Ejemplo: Card (1995)

Podemos acceder a las pruebas de diagnóstico usando la opción *diagnostics*

```{r echo=T, eval=F}
summary(vi, diagnostics=T)

```


---

class: inverse, middle, center

# Instrumentos débiles


---

# Instrumentos débiles

Discusión intuitiva en Angrist & Pischke (MHE, 2009)

El estimador de MCO tiene las propiedades de ser consistente e insesgado

En una muestra de tamaño arbitrario, la distribución del coeficiente de MCO está centrada en el coeficiente de  poblacional

En cambio, el estimador de MC2E, aunque consistente, **es sesgado**

En muestras grandes el el estimador está *cerca* del coeficiente poblacional

Esto tiene importantes consecuencias para la estimación y la inferencia


---

# Sesgo del estimador de MC2E

Consideremos el modelo simple con un solo regresor endógeno $y=\beta x+ \eta$

Supongamos que tenemos una matriz de instrumentos $Z$, por lo que la primera etapa es:

$$x=Z\pi+\xi$$

El estimador de MC2E es:

$$\hat{\beta}_{MC2E}=\beta+(x'P_Z x)^{-1}x'P_Z\eta$$

Sustituyendo $x$

$$\hat{\beta}_{MC2E}-\beta=(x'P_z x)^{-1}\pi'Z'\eta+(x'P_z x)^{-1}\xi'P_z\eta=sesgo_{Mc2E}$$

No podemos calcular directamente el sesgo pues el operador esperanza es un operador lineal

Angrist & Pischke (2009) aproximan el sesgo como.


$$E(\hat{\beta}_{MC2E}-\beta)\approx(E(x'P_z x))^{-1}E(\pi'Z'\eta)+(E(x'P_z x))^{-1}\xi'P_z\eta$$
---

# Sesgo del estimador de MC2E

La expresión del sesgo puede reescribirse como

$$E(\hat{\beta}_{MC2E}-\beta)\approx\frac{\sigma_{\eta\xi}}{\sigma_{xi}^2}\frac{1}{F+1}$$

donde $\frac{\sigma_{\eta \xi}}{\sigma_{xi}^2}$ es el sesgo del estimador de MCO

Cuando $\pi=0$, el sesgo de MC2E es el mismo que el de MCO

Es decir, cuando $F$ es pequeña, el sesgo de MC2E se acerca al sesgo de MCO: el estimador de MC2E está sesgado hacia el de MCO cuando la primera etapa es débil

Staiger & Stock (1997) mostraron con simulaciones que cuando $F>10$, el sesgo máximo en el estimador de MC2E es de 10%

De aquí viene la regla de dedo frecuentemente usada para juzgar instrumentos débiles


---

# Recomendaciones prácticas

1. Reportar la primera etapa y ver si los coeficientes tienen sentido económico

1. Reportar el estadístico $F$ de la primera etapa para los instrumentos excluidos

1. Reportar los resultados usando un modelo exactamente identificado usando el *mejor* instrumento

1. Reportar las prubeas de sobreidentificación, cuando sea posible

1. Poner atención a la forma reducida, recordando que la forma reducida es proporcional al efecto causal de interés

> "Si no puedes ver la relación causal de interés en la forma reducida es porque probablemente no haya nada ahí."
>
> --- Angrist & Krueger (2001)


---

class: center, middle
Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**